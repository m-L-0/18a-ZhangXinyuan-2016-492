LMS算法  
Rosenblatt感知器是第一个用于分类任务的神经网络模型，最小均方法是第一个使用神经网络模型完成回归任务的算法，LMS也可以作为其他回归算法的评价标准。  
LMS算法与Rosenblatt感知器类似，但结构更为简单，可以看做是一个神经元构成的感知器去掉了激活函数的形式，这样使得神经元的输出成为了连续值，等价于神经元的输入与输出之间进行了线性变换。  
LMS算法采用随机梯度下降法求解参数，也就是在训练中每次只计算一个样本的代价，并使用对应的梯度来更新参数。  
向前传播时，输入样本属性计算模型的输出，此时认为模型的参数是确定的，输入x是变量。当计算模型代价并更新参数时，此时认为输入的样本属性是确定的，参数w是变量。  

无约束最优化  
考虑代价函数，它是一个未知参数向量w连续可微的函数。  
很多神经网络模型适用一种以局部迭代下降的无约束最优化法，基本思想：从一个初始估计值w(0)开始，产生一系列权值向量w(1),w(2),...，使得代价函数J(w)在算法的每次迭代中都要下降。我们希望算法最终收敛到最优解w，但并不总能做到，有时算法会发散。  

最速下降法  
也被称为梯度下降法，使用最速下降法迭代求极值时，每次对权值向量w的调整都是在最速下降的方向进行的，即他是与梯度向量▽J(w)方向相反的。  

批量与小批量算法  
很多神经网络算法的代价函数通常可分为训练样本上的求和，在利用优化算法如梯度下降算法计算参数的每一次更新。准确计算每一次梯度的期望代价非常大，因为需要遍历整个数据集。通常使用小批量样本获得梯度的统计估计。这样做是因为使用更多样本来估计梯度的回报是小于线性的，其次由于训练集往往冗余，训练集中大多数样本对梯度做出了非常相似的贡献。  
  ·使用整个训练集的优化方法被称为批量或确定性算法；  
  ·小批量算法是指使用小批量训练集的优化算法；  
  ·每次只使用单个样本的优化算法被称为随机或在线算法；  
  ·在线算法通常是指从连续产生样本的数据流中抽取样本的情况；  
  ·随机算法通常是指从固定大小的训练集中遍历多次采样的情况；  
一般，我们认为梯度下降法就是指批梯度下降法，是使用全部样本计算代价和估计梯度的优化法，小批量梯度下降法是指随机使用数据集的一个固定大小的子集计算代价和估计梯度的优化法，随机梯度下降法是指随机使用数据集的一个固定大小的子集计算代价和估计梯度优化法。  
小批量数据是随机抽取的，要尽量避免连续样本之间具有高度的相关性，通常可以在抽取小批量样本时对样本顺序进行一次打乱；当没有重复使用样本时，小批量数据对梯度的估计是无偏的，它遵循着真实泛化误差的梯度。  
选择小批量的大小时往往要考虑很多因素。  
